{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (C-)DUSVGD for Sampling from a Mixture of Gaussian Distributions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam, SGD, RMSprop\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.distributions.mixture_same_family import MixtureSameFamily\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(edgeitems=1000)\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(\"torch.cuda.FloatTensor\" if torch.cuda.is_available() else \"torch.FloatTensor\")\n",
    "print(\"device =\", device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "$$p(x) = a_1\\mathcal{N}(\\mu_1,\\sigma_1^2) + a_2\\mathcal{N}(\\mu_2,\\sigma_2^2)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self):\n",
    "        self.weights = torch.tensor([0.75, 0.25])\n",
    "        self.means = torch.tensor([-2.0, 2.5])\n",
    "        self.stds = torch.tensor([1.0, 1.0])\n",
    "\n",
    "    def model(self):\n",
    "        mix = Categorical(self.weights)\n",
    "        comp = Normal(self.means, self.stds)\n",
    "        return MixtureSameFamily(mix, comp)\n",
    "\n",
    "    def print_parameters(self):\n",
    "        print('Target Distribution (GMM): weights =', self.weights.tolist(), \n",
    "              ', means =', self.means.tolist(), ', stds =', self.stds.tolist())\n",
    "\n",
    "model_gmm = GMM()\n",
    "target_model = model_gmm.model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for SVGD and C-DUSVGD\n",
    "num_particles = 100         # Number of particles in SVGD\n",
    "data_size = 1000            # Total number of data points\n",
    "train_batch_size = 50       # Batch size for training\n",
    "test_batch_size = 100       # Batch size for testing\n",
    "num_epochs = 10             # Number of epochs for DUSVGD training\n",
    "num_epochs_c = 40           # Number of epochs for C-DUSVGD training\n",
    "max_du_iterations = 10      # Number of iterations for dual updates in DUSVGD\n",
    "lr_adam = 0.005             # Learning rate for Adam optimizer in DUSVGD\n",
    "lr_adam_c = 0.0005          # Learning rate for Adam optimizer in C-DUSVGD\n",
    "init_params = 2.0           # Initial value for SVGD learning rate\n",
    "init_params_c = torch.tensor([0.3, 1.0])  # Initial parameters for C-DUSVGD\n",
    "init_dist_mean = -2.0       # Mean of the initial distribution for particles\n",
    "init_dist_std = 1.0         # Standard deviation of the initial distribution for particles\n",
    "\n",
    "def print_hyperparameters():\n",
    "    print('num_particles:\\t', num_particles)\n",
    "    print('data_size:\\t', data_size)\n",
    "    print('train_batch_size:\\t', train_batch_size)\n",
    "    print('test_batch_size:\\t', test_batch_size)\n",
    "    print('num_epochs:\\t', num_epochs)\n",
    "    print('num_epochs_c:\\t', num_epochs_c)\n",
    "    print('max_du_iterations:\\t', max_du_iterations)\n",
    "    print('lr_adam:\\t', lr_adam)\n",
    "    print('lr_adam_c:\\t', lr_adam_c)\n",
    "    print('init_params:\\t', init_params)\n",
    "    print('init_params_c:\\t', init_params_c.tolist())\n",
    "    print('init_distribution (Normal): mean =', init_dist_mean, ', std =', init_dist_std)\n",
    "    model_gmm.print_parameters()\n",
    "\n",
    "print_hyperparameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.normal(init_dist_mean, init_dist_std, size=(data_size, num_particles)).to(device)\n",
    "t_data = target_model.sample([data_size, num_particles]).to(device)\n",
    "\n",
    "train_size = int(0.9 * data_size)\n",
    "test_size = data_size - train_size\n",
    "x_train, x_test = torch.utils.data.random_split(x_data, [train_size, test_size], generator=torch.Generator(device))\n",
    "t_train, t_test = torch.utils.data.random_split(t_data, [train_size, test_size], generator=torch.Generator(device))\n",
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, X, t):\n",
    "        self.X = X\n",
    "        self.t = t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.t[index]\n",
    "\n",
    "train_dataset = CustomDataset(x_train, t_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, generator=torch.Generator(device))\n",
    "\n",
    "test_dataset = CustomDataset(x_test, t_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True, generator=torch.Generator(device))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(tensor):\n",
    "    tensor = tensor.clone().flatten(1)\n",
    "    tensor_max = tensor.max(1).values.unsqueeze(1)\n",
    "    median_val = (torch.cat((tensor, tensor_max), 1).median(1).values + tensor.median(1).values) / 2.0\n",
    "    return median_val.view(-1, 1, 1)\n",
    "\n",
    "def get_gradient(model_gmm, inputs, alpha, beta, retain_graph):\n",
    "    inputs = inputs.clone().requires_grad_(True)\n",
    "    bs, n, d = inputs.shape\n",
    "\n",
    "    log_prob = model_gmm.model().log_prob(inputs)\n",
    "    log_prob_grad = torch.autograd.grad(log_prob.sum(), inputs, retain_graph=retain_graph)[0]\n",
    "\n",
    "    pairwise_distance = torch.cdist(inputs, inputs, p=2.0).pow(2).to(device)\n",
    "    h = median(pairwise_distance) / math.log(n)\n",
    "    kernel = torch.exp(-pairwise_distance / h)\n",
    "    kernel_grad = 2 * (kernel.sum(2).diag_embed() - kernel).matmul(inputs) / h\n",
    "\n",
    "    gradient = -(alpha * torch.matmul(kernel, log_prob_grad) + beta * kernel_grad) / n\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUSVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DUGD(nn.Module):\n",
    "    def __init__(self, itr):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_params * torch.ones(itr))\n",
    "\n",
    "    def forward(self, iteration, x_data):\n",
    "        for i in range(iteration):\n",
    "            j = i % max_du_iterations\n",
    "            x_data = x_data - abs(self.gamma[j]) * get_gradient(model_gmm, x_data, 1.0, 1.0, retain_graph=True)\n",
    "\n",
    "        return x_data, self.gamma\n",
    "\n",
    "model_dugd = DUGD(max_du_iterations).to(device)\n",
    "opt_dugd = optim.Adam(model_dugd.parameters(), lr=lr_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-DUSVGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev_step(a, b, t, T):\n",
    "    la1 = a ** 2\n",
    "    lan = a ** 2 + b ** 2\n",
    "    lr_c = 1 / ((la1 + lan) / 2 + ((lan - la1) / 2) * math.cos((2 * (T - t) - 1) * torch.pi / (2 * T)))\n",
    "    return lr_c\n",
    "\n",
    "class CDUGD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_params_c)\n",
    "\n",
    "    def forward(self, iteration, x_data):\n",
    "        for i in range(iteration):\n",
    "            lr_c = chebyshev_step(self.gamma[0], self.gamma[1], i, max_du_iterations)\n",
    "            x_data = x_data - lr_c * get_gradient(model_gmm, x_data, 1.0, 1.0, retain_graph=True)\n",
    "\n",
    "        return x_data, self.gamma\n",
    "\n",
    "model_cdugd = CDUGD().to(device)\n",
    "opt_cdugd = optim.Adam(model_cdugd.parameters(), lr=lr_adam_c)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        x, y = outputs, targets\n",
    "        bs, n, d = x.shape\n",
    "        _, m, _ = y.shape\n",
    "        sigma = 1\n",
    "\n",
    "        xy = torch.cat([x.clone(), y.clone()], dim=1).to(device)\n",
    "        dists = torch.cdist(xy, xy, p=2.0)\n",
    "        k = torch.exp(-0.5 * dists**2) + torch.ones_like(dists) * 1e-5\n",
    "\n",
    "        k_x = k[:, :n, :n]\n",
    "        k_y = k[:, n:, n:]\n",
    "        k_xy = k[:, :n, n:]\n",
    "        mmd = (k_x.sum() / (n * (n - 1)) + k_y.sum() / (m * (m - 1)) - 2 * k_xy.sum() / (n * m)) / bs\n",
    "        return mmd\n",
    "\n",
    "loss_func = MMDLoss()\n",
    "\n",
    "def incremental_training():\n",
    "    for iteration in range(max_du_iterations):\n",
    "        print()\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in train_dataloader:\n",
    "                torch.autograd.set_detect_anomaly(True)\n",
    "                x, t = [tensor.unsqueeze(2).to(device) for tensor in batch]\n",
    "\n",
    "                opt_dugd.zero_grad()\n",
    "                x_hat, gamma = model_dugd(iteration + 1, x)\n",
    "                loss = loss_func(x_hat, t)\n",
    "                loss.backward()\n",
    "                opt_dugd.step()\n",
    "\n",
    "            print(f\"\\ri: {iteration + 1}\\te: {epoch + 1}\\tparams: {', '.join(f'{g:.2f}' for g in gamma.tolist())}\\tloss: {loss.item():.3f}\", end=\"    \")\n",
    "\n",
    "def static_training():\n",
    "    for epoch in range(num_epochs_c):\n",
    "        for batch in train_dataloader:\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "            x, t = [tensor.unsqueeze(2).to(device) for tensor in batch]\n",
    "\n",
    "            opt_cdugd.zero_grad()\n",
    "            x_hat, gamma = model_cdugd(max_du_iterations, x)\n",
    "            loss = loss_func(x_hat, t)\n",
    "            loss.backward()\n",
    "            opt_cdugd.step()\n",
    "\n",
    "        print(f\"\\re: {epoch + 1}\\tparams: {', '.join(f'{g:.2f}' for g in gamma.tolist())}\\tloss: {loss.item():.3f}\", end=\"    \")\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute SVGD and Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dugd = model_dugd.gamma\n",
    "lr_cdugd = model_cdugd.gamma\n",
    "\n",
    "D, par, iteration_ = 4, max_du_iterations, 500\n",
    "mmds = np.zeros((D, iteration_ // par))\n",
    "lr_c = np.array([chebyshev_step(lr_cdugd[0], lr_cdugd[1], i, max_du_iterations).item() for i in range(max_du_iterations)])\n",
    "\n",
    "lr_fixed = 2.0\n",
    "lr_RMSprop = 0.002\n",
    "lr_Adam = 0.02\n",
    "\n",
    "# Dual Gradient Descent (DUGD) step for particles\n",
    "def dugd_step(particles, lr_val):\n",
    "    particles -= lr_val * get_gradient(model_gmm, particles, 1.0, 1.0, retain_graph=False)\n",
    "    return particles\n",
    "\n",
    "# Chebyshev Step Gradient Descent (C-DUGD) step for particles\n",
    "def cdugd_step(particles, lr_val):\n",
    "    particles -= lr_val * get_gradient(model_gmm, particles, 1.0, 1.0, retain_graph=False)\n",
    "    return particles\n",
    "\n",
    "# RMSprop optimization step for particles\n",
    "def rmsprop_step(particles, lr_val=lr_RMSprop):\n",
    "    optimizer = RMSprop([particles], lr=lr_val)\n",
    "    optimizer.zero_grad()\n",
    "    particles.grad = get_gradient(model_gmm, particles, 1.0, 1.0, retain_graph=False)\n",
    "    optimizer.step()\n",
    "    return particles\n",
    "\n",
    "# Fixed-step update for particles\n",
    "def fixed_step(particles, lr_val=lr_fixed):\n",
    "    particles -= lr_val * get_gradient(model_gmm, particles, 1.0, 1.0, retain_graph=False)\n",
    "    return particles\n",
    "\n",
    "def test():\n",
    "    global mmds, lr_c\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        init_particles, target_particles = [tensor.unsqueeze(2) for tensor in batch]\n",
    "        break\n",
    "\n",
    "    particles = [init_particles.clone() for _ in range(D)]\n",
    "    idx = 0\n",
    "\n",
    "    for i in range(iteration_):\n",
    "        print(f\"\\r{i / iteration_ * 100:.0f}% complete\", end=\"\")\n",
    "        \n",
    "        # Every `par` iterations, record the MMD between particles and target\n",
    "        if i % par == 0:\n",
    "            with torch.no_grad():\n",
    "                for d in range(D):\n",
    "                    mmds[d, idx] = loss_func(particles[d], target_particles).item()\n",
    "                idx += 1\n",
    "\n",
    "        j = i % max_du_iterations\n",
    "        particles[0] = dugd_step(particles[0], lr_dugd[j])\n",
    "        particles[1] = cdugd_step(particles[1], lr_c[j])\n",
    "        particles[2] = rmsprop_step(particles[2])\n",
    "        particles[3] = fixed_step(particles[3])\n",
    "\n",
    "    return particles\n",
    "\n",
    "particles_list = test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results of SVGD Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print hyperparameters to verify settings\n",
    "print_hyperparameters()\n",
    "\n",
    "# Plotting function for results\n",
    "def plotf():\n",
    "    particles = [p.detach().flatten().cpu().numpy() for p in particles_list]\n",
    "    \n",
    "    print(\"\\nLearned Step Sizes:\")\n",
    "    print(\"DUSVGD:\", lr_dugd)\n",
    "    print(\"C-DUSVGD:\", lr_cdugd)\n",
    "\n",
    "    w, w2 = 2, 2 #linewidth\n",
    "\n",
    "    plt.figure(dpi=300)\n",
    "    target = target_model.sample([10000])\n",
    "    sns.kdeplot(target.squeeze().detach().cpu().numpy(), linewidth=w2, bw_method=0.2, color=\"black\", label=\"Target\")\n",
    "    \n",
    "    styles = [\"dashed\", \"dashdot\", \"dotted\", (0, (1, 1))]\n",
    "    colors = [\"red\", \"blue\", \"orange\", \"green\", \"purple\"]\n",
    "    labels = [\"DUSVGD:Proposed\", \"C-DUSVGD:Proposed\", \"RMSProp\", \"Fixed step size\"]\n",
    "\n",
    "    for i, (p, style, color, label) in enumerate(zip(particles, styles, colors, labels)):\n",
    "        sns.kdeplot(p, linewidth=w, bw_method=0.2, linestyle=style, color=color, label=label)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Particle Distributions Compared to Target\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(dpi=300)\n",
    "    x_plt = np.arange(0, iteration_, max_du_iterations)\n",
    "    for d in range(D):\n",
    "        plt.plot(x_plt, np.log10(mmds[d]), linewidth=w, linestyle=styles[d], color=colors[d], label=labels[d])\n",
    "\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Log10 MMD\")\n",
    "    plt.title(\"Log MMD over Iterations\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(dpi=300)\n",
    "    plt_x = np.arange(0, max_du_iterations)\n",
    "    plt_dugd_step = lr_dugd.detach().cpu().numpy()\n",
    "    plt_cdugd_step = lr_c\n",
    "\n",
    "    plt.plot(plt_x, plt_dugd_step, marker=\".\", color=\"red\", linewidth=w, label=\"DUSVGD\")\n",
    "    plt.plot(plt_x, np.sort(plt_dugd_step)[::-1], marker=\"o\", color=\"red\", alpha=0.2, linewidth=w, label=\"DUSVGD Sorted\")\n",
    "    plt.plot(plt_x, plt_cdugd_step, marker=\".\", color=\"blue\", linewidth=w, label=\"C-DUSVGD\")\n",
    "    \n",
    "    plt.xlabel(\"Index t\")\n",
    "    plt.ylabel(\"Step Size\")\n",
    "    plt.title(\"Learned Step Sizes for DUSVGD and C-DUSVGD\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
